{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1105f8d5-dc46-4998-8042-b5e4991d7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "##Orvieto A, Raj A, Kersting H, Bach F. Explicit regularization in overparametrized#\n",
    "##models via noise injection. 2023, arXiv, Available: https://arxiv.org/pdf/2206.###\n",
    "###https://github.com/aorvieto/noise_injection_overparam ###########################\n",
    "####################################################################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import time\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "#from utils import flat_params, compute_jacobian, get_lr\n",
    "#from models import CIFARCNN2, CIFARCNN3, MNISTNet1, MNISTNet2, MNISTNet3, CIFARCNN1, vgg11_bn, ResNet34, ResNet18\n",
    "from datetime import datetime\n",
    "from pyhessian import hessian\n",
    "import torchvision.transforms as tt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c13b509-5a99-4b56-913c-089d57be8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据flat成一维\n",
    "def flat_params(m):\n",
    "    flat_data = []\n",
    "    for p in m.parameters():\n",
    "        flat_data.append(p.data.view(-1))\n",
    "    return torch.cat(flat_data)\n",
    "\n",
    "#梯度的范数\n",
    "def grad_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.detach().data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    return total_norm.cpu() ** 0.5\n",
    "\n",
    "#零梯度\n",
    "def zero_gradients(x):\n",
    "    if x.grad is not None:\n",
    "    \tx.grad.zero_()\n",
    "\n",
    "#设置学习率\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "#计算Jacobian矩阵\n",
    "def compute_jacobian(inputs, output):\n",
    "\t\"\"\"\n",
    "\t:param inputs: Batch X Size (e.g. Depth X Width X Height)\n",
    "\t:param output: Batch X Classes\n",
    "\t:return: jacobian: Batch X Classes X Size\n",
    "\t\"\"\"\n",
    "\tassert inputs.requires_grad\n",
    "\n",
    "\tnum_classes = output.size()[1]\n",
    "\n",
    "\tjacobian = torch.zeros(num_classes, *inputs.size())\n",
    "\tgrad_output = torch.zeros(*output.size())\n",
    "\tif inputs.is_cuda:\n",
    "\t\tgrad_output = grad_output.cuda()\n",
    "\t\tjacobian = jacobian.cuda()\n",
    "\n",
    "\tfor i in range(num_classes):\n",
    "\t\tzero_gradients(inputs)\n",
    "\t\tgrad_output.zero_()\n",
    "\t\tgrad_output[:, i] = 1\n",
    "\t\toutput.backward(grad_output, retain_variables=True)\n",
    "\t\tjacobian[i] = inputs.grad.data\n",
    "\n",
    "\treturn torch.transpose(jacobian, dim0=0, dim1=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1465a48-b759-4c82-80b9-95bc87133b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "####定义了很多网络结构\n",
    "####全连接网络，卷积网，VGG，残差ResN\n",
    "\n",
    "\n",
    "class MNISTNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet1, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(500, 500)\n",
    "        self.fc4 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "class MNISTNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet2, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 5000)\n",
    "        self.fc2 = nn.Linear(5000, 5000)\n",
    "        self.fc3 = nn.Linear(5000, 5000)\n",
    "        self.fc4 = nn.Linear(5000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "class MNISTNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet3, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 1000)\n",
    "        self.fc5 = nn.Linear(1000, 1000)\n",
    "        self.fc6 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        return F.log_softmax(self.fc6(x), dim=1)\n",
    "\n",
    "class CIFARCNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class CIFARCNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 128, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x))) \n",
    "        x = x.mean(dim=[2,3]) \n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class CIFARCNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 128, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(128, 5000)\n",
    "        self.fc2 = nn.Linear(5000, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x))) \n",
    "        x = x.mean(dim=[2,3]) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x       \n",
    "\n",
    "### CIFAR 100 stuff ###\n",
    "cfg = {\n",
    "    'A' : [64,     'M', 128,      'M', 256, 256,           'M', 512, 512,           'M', 512, 512,           'M'],\n",
    "    'B' : [64, 64, 'M', 128, 128, 'M', 256, 256,           'M', 512, 512,           'M', 512, 512,           'M'],\n",
    "    'D' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256,      'M', 512, 512, 512,      'M', 512, 512, 512,      'M'],\n",
    "    'E' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, num_class=100):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.features(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.classifier(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "\n",
    "    input_channel = 3\n",
    "    for l in cfg:\n",
    "        if l == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            continue\n",
    "\n",
    "        layers += [nn.Conv2d(input_channel, l, kernel_size=3, padding=1)]\n",
    "\n",
    "        if batch_norm:\n",
    "            layers += [nn.BatchNorm2d(l)]\n",
    "\n",
    "        layers += [nn.ReLU(inplace=True)]\n",
    "        input_channel = l\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg11_bn():\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
    "\n",
    "def vgg13_bn():\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "def vgg16_bn():\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "def vgg19_bn():\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))\n",
    "\n",
    "##### REs CIFAR 100\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "        if False:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "        if False:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out_final = self.linear(out)\n",
    "        return out_final \n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "def ResNet200():\n",
    "    return ResNet(Bottleneck, [3, 24, 36, 3])\n",
    "\n",
    "def ResNet270():\n",
    "    return ResNet(Bottleneck, [3,36,48,3])\n",
    "\n",
    "def ResNet336():\n",
    "    return ResNet(Bottleneck, [3,44,62,3])\n",
    "\n",
    "def ResNet500():\n",
    "    return ResNet(Bottleneck, [3,70,90,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36946bd3-a0fd-45f4-bb16-fe15d4fb04a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model dimension: 11173962\n",
      "Number of groups: 62\n",
      "epoch #0, lr = 0.01\n",
      "Epoch 0: Train L: 2.3023, TrainAcc: 9.72, Test L: 2.3018, TestAcc: 9.41 \n",
      "\n",
      "89.8532280921936\n",
      "epoch #1, lr = 0.009990133642141357\n",
      "Epoch 1: Train L: 1.1323, TrainAcc: 59.62, Test L: 1.1449, TestAcc: 58.97 \n",
      "\n",
      "53.822288036346436\n",
      "epoch #2, lr = 0.009960573506572389\n",
      "Epoch 2: Train L: 0.8713, TrainAcc: 69.37, Test L: 0.9118, TestAcc: 68.65 \n",
      "\n",
      "57.694769859313965\n",
      "epoch #3, lr = 0.009911436253643442\n",
      "Epoch 3: Train L: 0.7879, TrainAcc: 72.45, Test L: 0.8116, TestAcc: 71.55 \n",
      "\n",
      "66.92317152023315\n",
      "epoch #4, lr = 0.009842915805643154\n",
      "Epoch 4: Train L: 0.6334, TrainAcc: 78.14, Test L: 0.6977, TestAcc: 76.77 \n",
      "\n",
      "76.02175617218018\n",
      "epoch #5, lr = 0.009755282581475767\n",
      "Epoch 5: Train L: 0.5502, TrainAcc: 80.60, Test L: 0.6188, TestAcc: 79.07 \n",
      "\n",
      "85.50759744644165\n",
      "epoch #6, lr = 0.009648882429441256\n",
      "Epoch 6: Train L: 0.4803, TrainAcc: 83.66, Test L: 0.5395, TestAcc: 82.24 \n",
      "\n",
      "91.62318468093872\n",
      "epoch #7, lr = 0.009524135262330098\n",
      "Epoch 7: Train L: 0.5368, TrainAcc: 81.51, Test L: 0.6032, TestAcc: 80.93 \n",
      "\n",
      "98.92474675178528\n",
      "epoch #8, lr = 0.009381533400219317\n",
      "Epoch 8: Train L: 0.4278, TrainAcc: 85.33, Test L: 0.5395, TestAcc: 83.23 \n",
      "\n",
      "100.71943593025208\n",
      "epoch #9, lr = 0.009221639627510075\n",
      "Epoch 9: Train L: 0.3744, TrainAcc: 86.80, Test L: 0.4688, TestAcc: 84.47 \n",
      "\n",
      "107.10489535331726\n",
      "epoch #10, lr = 0.009045084971874737\n",
      "Epoch 10: Train L: 0.3582, TrainAcc: 87.39, Test L: 0.4484, TestAcc: 85.15 \n",
      "\n",
      "106.79664397239685\n",
      "epoch #11, lr = 0.008852566213878947\n",
      "Epoch 11: Train L: 0.2936, TrainAcc: 89.77, Test L: 0.3976, TestAcc: 86.76 \n",
      "\n",
      "111.5773983001709\n",
      "epoch #12, lr = 0.008644843137107058\n",
      "Epoch 12: Train L: 0.2827, TrainAcc: 90.26, Test L: 0.4273, TestAcc: 86.40 \n",
      "\n",
      "111.68043255805969\n",
      "epoch #13, lr = 0.008422735529643444\n",
      "Epoch 13: Train L: 0.3155, TrainAcc: 89.02, Test L: 0.4341, TestAcc: 85.74 \n",
      "\n",
      "113.830894947052\n",
      "epoch #14, lr = 0.00818711994874345\n",
      "Epoch 14: Train L: 0.2335, TrainAcc: 91.92, Test L: 0.3600, TestAcc: 87.92 \n",
      "\n",
      "114.90329933166504\n",
      "epoch #15, lr = 0.007938926261462368\n",
      "Epoch 15: Train L: 0.2316, TrainAcc: 91.92, Test L: 0.3686, TestAcc: 88.02 \n",
      "\n",
      "116.48366641998291\n",
      "epoch #16, lr = 0.007679133974894984\n",
      "Epoch 16: Train L: 0.2265, TrainAcc: 92.17, Test L: 0.3343, TestAcc: 88.59 \n",
      "\n",
      "117.16001105308533\n",
      "epoch #17, lr = 0.007408768370508577\n",
      "Epoch 17: Train L: 0.1750, TrainAcc: 93.97, Test L: 0.3193, TestAcc: 89.49 \n",
      "\n",
      "123.29750323295593\n",
      "epoch #18, lr = 0.0071288964578253644\n",
      "Epoch 18: Train L: 0.2163, TrainAcc: 92.29, Test L: 0.4139, TestAcc: 87.42 \n",
      "\n",
      "128.9900517463684\n",
      "epoch #19, lr = 0.0068406227634233915\n",
      "Epoch 19: Train L: 0.1596, TrainAcc: 94.48, Test L: 0.3168, TestAcc: 89.39 \n",
      "\n",
      "130.32004737854004\n",
      "epoch #20, lr = 0.006545084971874738\n",
      "Epoch 20: Train L: 0.1753, TrainAcc: 93.85, Test L: 0.3449, TestAcc: 88.99 \n",
      "\n",
      "125.49934768676758\n",
      "epoch #21, lr = 0.006243449435824274\n",
      "Epoch 21: Train L: 0.1933, TrainAcc: 92.99, Test L: 0.3839, TestAcc: 88.57 \n",
      "\n",
      "131.1132252216339\n",
      "epoch #22, lr = 0.005936906572928625\n",
      "Epoch 22: Train L: 0.1575, TrainAcc: 94.39, Test L: 0.3712, TestAcc: 88.65 \n",
      "\n",
      "133.01935052871704\n",
      "epoch #23, lr = 0.005626666167821524\n",
      "Epoch 23: Train L: 0.1637, TrainAcc: 94.03, Test L: 0.3544, TestAcc: 88.76 \n",
      "\n",
      "128.67377829551697\n",
      "epoch #24, lr = 0.005313952597646569\n",
      "Epoch 24: Train L: 0.1106, TrainAcc: 96.14, Test L: 0.3230, TestAcc: 89.82 \n",
      "\n",
      "128.1248128414154\n",
      "epoch #25, lr = 0.005000000000000002\n",
      "Epoch 25: Train L: 0.1161, TrainAcc: 95.85, Test L: 0.3509, TestAcc: 90.35 \n",
      "\n",
      "128.50095891952515\n",
      "epoch #26, lr = 0.004686047402353435\n",
      "Epoch 26: Train L: 0.1072, TrainAcc: 96.62, Test L: 0.2916, TestAcc: 90.53 \n",
      "\n",
      "132.07407307624817\n",
      "epoch #27, lr = 0.004373333832178481\n",
      "Epoch 27: Train L: 0.0818, TrainAcc: 97.19, Test L: 0.3149, TestAcc: 90.56 \n",
      "\n",
      "135.4850480556488\n",
      "epoch #28, lr = 0.0040630934270713785\n",
      "Epoch 28: Train L: 0.0845, TrainAcc: 97.04, Test L: 0.3345, TestAcc: 90.95 \n",
      "\n",
      "133.2352454662323\n",
      "epoch #29, lr = 0.0037565505641757274\n",
      "Epoch 29: Train L: 0.0963, TrainAcc: 96.86, Test L: 0.3066, TestAcc: 90.37 \n",
      "\n",
      "132.09872364997864\n",
      "epoch #30, lr = 0.0034549150281252662\n",
      "Epoch 30: Train L: 0.0705, TrainAcc: 97.51, Test L: 0.3404, TestAcc: 90.72 \n",
      "\n",
      "129.56700086593628\n",
      "epoch #31, lr = 0.0031593772365766134\n",
      "Epoch 31: Train L: 0.0498, TrainAcc: 98.40, Test L: 0.3080, TestAcc: 91.38 \n",
      "\n",
      "130.23395705223083\n",
      "epoch #32, lr = 0.0028711035421746384\n",
      "Epoch 32: Train L: 0.0437, TrainAcc: 98.77, Test L: 0.2834, TestAcc: 91.85 \n",
      "\n",
      "129.16136765480042\n",
      "epoch #33, lr = 0.002591231629491424\n",
      "Epoch 33: Train L: 0.0603, TrainAcc: 97.80, Test L: 0.3153, TestAcc: 91.37 \n",
      "\n",
      "128.58228492736816\n",
      "epoch #34, lr = 0.0023208660251050166\n",
      "Epoch 34: Train L: 0.0326, TrainAcc: 99.01, Test L: 0.2969, TestAcc: 91.96 \n",
      "\n",
      "130.6975016593933\n",
      "epoch #35, lr = 0.0020610737385376356\n",
      "Epoch 35: Train L: 0.0281, TrainAcc: 99.19, Test L: 0.3185, TestAcc: 91.89 \n",
      "\n",
      "129.7619194984436\n",
      "epoch #36, lr = 0.001812880051256552\n",
      "Epoch 36: Train L: 0.0246, TrainAcc: 99.49, Test L: 0.2665, TestAcc: 92.14 \n",
      "\n",
      "131.45330572128296\n",
      "epoch #37, lr = 0.001577264470356557\n",
      "Epoch 37: Train L: 0.0192, TrainAcc: 99.50, Test L: 0.2862, TestAcc: 92.49 \n",
      "\n",
      "133.32038116455078\n",
      "epoch #38, lr = 0.0013551568628929439\n",
      "Epoch 38: Train L: 0.0161, TrainAcc: 99.60, Test L: 0.3028, TestAcc: 92.31 \n",
      "\n",
      "132.03708720207214\n",
      "epoch #39, lr = 0.0011474337861210548\n",
      "Epoch 39: Train L: 0.0274, TrainAcc: 99.45, Test L: 0.2577, TestAcc: 92.29 \n",
      "\n",
      "133.97796654701233\n",
      "epoch #40, lr = 0.0009549150281252636\n",
      "Epoch 40: Train L: 0.0137, TrainAcc: 99.69, Test L: 0.2882, TestAcc: 92.48 \n",
      "\n",
      "139.91855764389038\n",
      "epoch #41, lr = 0.000778360372489926\n",
      "Epoch 41: Train L: 0.0112, TrainAcc: 99.76, Test L: 0.2993, TestAcc: 92.61 \n",
      "\n",
      "137.90931057929993\n",
      "epoch #42, lr = 0.0006184665997806823\n",
      "Epoch 42: Train L: 0.0114, TrainAcc: 99.79, Test L: 0.2839, TestAcc: 92.56 \n",
      "\n",
      "135.9921793937683\n",
      "epoch #43, lr = 0.0004758647376699034\n",
      "Epoch 43: Train L: 0.0107, TrainAcc: 99.82, Test L: 0.2798, TestAcc: 92.87 \n",
      "\n",
      "134.50317478179932\n",
      "epoch #44, lr = 0.0003511175705587434\n",
      "Epoch 44: Train L: 0.0087, TrainAcc: 99.86, Test L: 0.2968, TestAcc: 92.68 \n",
      "\n",
      "133.63954830169678\n",
      "epoch #45, lr = 0.00024471741852423245\n",
      "Epoch 45: Train L: 0.0111, TrainAcc: 99.84, Test L: 0.2794, TestAcc: 92.70 \n",
      "\n",
      "134.66607928276062\n",
      "epoch #46, lr = 0.00015708419435684522\n",
      "Epoch 46: Train L: 0.0101, TrainAcc: 99.88, Test L: 0.2657, TestAcc: 92.83 \n",
      "\n",
      "133.50323343276978\n",
      "epoch #47, lr = 8.856374635655642e-05\n",
      "Epoch 47: Train L: 0.0081, TrainAcc: 99.90, Test L: 0.2865, TestAcc: 92.68 \n",
      "\n",
      "132.9155330657959\n",
      "epoch #48, lr = 3.942649342761119e-05\n",
      "Epoch 48: Train L: 0.0088, TrainAcc: 99.81, Test L: 0.3033, TestAcc: 92.79 \n",
      "\n",
      "134.7095489501953\n",
      "epoch #49, lr = 9.866357858642208e-06\n",
      "Epoch 49: Train L: 0.0119, TrainAcc: 99.89, Test L: 0.2543, TestAcc: 92.82 \n",
      "\n",
      "136.85899877548218\n"
     ]
    }
   ],
   "source": [
    "def train_net(settings):\n",
    "\n",
    "    ########### Setting Up GPU ###########  \n",
    "   #gpu_ids = GPU\n",
    "    #torch.cuda.set_device(gpu_ids[0])\n",
    "    device = 'cuda'\n",
    "\n",
    "    ########### Setup Data and Model ###########    \n",
    "    if settings[\"dataset\"]==\"MNIST\":\n",
    "\n",
    "        #data\n",
    "        train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "        validation_dataset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=settings[\"bs\"], shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True)\n",
    "        validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=settings[\"bs\"], shuffle=False, num_workers=8, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "        #model\n",
    "        if settings[\"net\"] == \"MLP1\": \n",
    "            model = torch.nn.DataParallel(MNISTNet1()).cuda()\n",
    "        elif settings[\"net\"] == \"MLP2\":\n",
    "            model = torch.nn.DataParallel(MNISTNet2()).cuda()\n",
    "        elif settings[\"net\"] == \"MLP3\":\n",
    "            model = torch.nn.DataParallel(MNISTNet3()).cuda()\n",
    "        else: print(\"model not defined\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    elif settings[\"dataset\"]==\"FMNIST\":\n",
    "\n",
    "        #data\n",
    "        train_dataset = datasets.FashionMNIST('./data',download=True, train= True, transform=transforms.ToTensor())\n",
    "        validation_dataset = datasets.FashionMNIST('./data',download=True, train= False, transform=transforms.ToTensor())\n",
    "\n",
    "        #Trainloader subset\n",
    "        #train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=settings[\"bs\"], shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True)\n",
    "        subset = random.sample(range(train_dataset.data.shape[0]),settings[\"subset\"])\n",
    "        sample_ds = torch.utils.data.Subset(train_dataset, subset)\n",
    "        sample_sampler = torch.utils.data.RandomSampler(sample_ds)\n",
    "        train_loader = torch.utils.data.DataLoader(sample_ds, sampler=sample_sampler, batch_size=settings[\"bs\"], num_workers=8, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "        #testloader\n",
    "        validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=settings[\"bs\"], shuffle=False, num_workers=8, pin_memory=True, persistent_workers=True)       \n",
    "\n",
    "        #hessloader subset\n",
    "        hess_loader = train_loader\n",
    "\n",
    "        #models\n",
    "        if settings[\"net\"] == \"MLP1\": \n",
    "            model = torch.nn.DataParallel(MNISTNet1()).cuda()\n",
    "        elif settings[\"net\"] == \"MLP2\":\n",
    "            model = torch.nn.DataParallel(MNISTNet2()).cuda()\n",
    "        elif settings[\"net\"] == \"MLP3\":\n",
    "            model = torch.nn.DataParallel(MNISTNet3()).cuda()\n",
    "        else: print(\"model not defined\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    elif settings[\"dataset\"]==\"CIFAR\":\n",
    "\n",
    "        #data transforms\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "\n",
    "        #data\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root='./data', download=True, train=True, transform=transform_train)\n",
    "        validation_dataset = torchvision.datasets.CIFAR10(root='./data', download=True, train=False, transform=transform_test)\n",
    "        \n",
    "        #trainloader subset\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=settings[\"bs\"], shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "        #subset = random.sample(range(train_dataset.data.shape[0]),settings[\"subset\"])\n",
    "        #sample_ds = torch.utils.data.Subset(train_dataset, subset)\n",
    "        #sample_sampler = torch.utils.data.RandomSampler(sample_ds)\n",
    "        #train_loader = torch.utils.data.DataLoader(sample_ds, sampler=sample_sampler, batch_size=settings[\"bs\"], num_workers=8, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "        #hessloader subset\n",
    "        subset = random.sample(range(train_dataset.data.shape[0]),int(settings[\"subset\"]/10))\n",
    "        sample_ds_hess = torch.utils.data.Subset(train_dataset, subset)\n",
    "        sample_sampler_hess = torch.utils.data.RandomSampler(sample_ds_hess)\n",
    "        hess_loader = torch.utils.data.DataLoader(sample_ds_hess, sampler=sample_sampler_hess, batch_size=settings[\"bs\"], num_workers=8, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "        #testloader\n",
    "        validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=settings[\"bs\"], shuffle=False, num_workers=8, pin_memory=True, persistent_workers=True)       \n",
    "\n",
    "        #models\n",
    "        if settings[\"net\"] == \"CNN1\": \n",
    "            model = torch.nn.DataParallel(CIFARCNN1()).cuda()\n",
    "        elif settings[\"net\"] == \"CNN2\": \n",
    "            model = torch.nn.DataParallel(CIFARCNN2()).cuda()\n",
    "        elif settings[\"net\"] == \"CNN3\": \n",
    "            model = torch.nn.DataParallel(CIFARCNN3()).cuda()\n",
    "        elif settings[\"net\"] == \"CIFAR10Res18\":\n",
    "            model = torch.nn.DataParallel(ResNet18()).cuda()\n",
    "        elif settings[\"net\"] == \"CIFAR10Res34\":\n",
    "            model = torch.nn.DataParallel(ResNet34()).cuda()\n",
    "\n",
    "        else: print(\"model not defined\")\n",
    "        criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "    elif settings[\"dataset\"]==\"CIFAR100\":\n",
    "        stats = ((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025))\n",
    "        train_transform = tt.Compose([tt.RandomHorizontalFlip(),tt.RandomCrop(32,padding=4,padding_mode=\"reflect\"),tt.ToTensor(), tt.Normalize(*stats)])\n",
    "        test_transform = tt.Compose([tt.ToTensor(),tt.Normalize(*stats)])\n",
    "        train_dataset = CIFAR100(download=True,root=\"./data\",transform=train_transform)\n",
    "        test_data = CIFAR100(root=\"./data\",train=False,transform=test_transform)\n",
    "\n",
    "        #trainloader subset\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=settings[\"bs\"],num_workers=8,pin_memory=True,shuffle=True, persistent_workers=True)\n",
    "       \n",
    "        #hessloader\n",
    "        subset = random.sample(range(train_dataset.data.shape[0]),int(settings[\"subset\"]/25))\n",
    "        sample_ds_hess = torch.utils.data.Subset(train_dataset, subset)\n",
    "        sample_sampler_hess = torch.utils.data.RandomSampler(sample_ds_hess)    \n",
    "        hess_loader = torch.utils.data.DataLoader(sample_ds_hess, sampler=sample_sampler_hess, batch_size=settings[\"bs\"], num_workers=8, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "        #testloader       \n",
    "        validation_loader = torch.utils.data.DataLoader(test_data,batch_size=settings[\"bs\"],num_workers=8,pin_memory=True, persistent_workers=True)\n",
    "\n",
    "        #models\n",
    "        if settings[\"net\"] == \"CIFAR100vgg\": \n",
    "            model = torch.nn.DataParallel(vgg11_bn()).cuda()\n",
    "        else: print(\"model not defined\")\n",
    "        criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "    # elif settings[\"dataset\"]==\"TinyImagenet\":\n",
    "    #     #todo\n",
    "\n",
    "\n",
    "    ########### Setup Optimizer ###########   \n",
    "    if settings[\"optimizer\"]==\"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=settings[\"lr\"],momentum=0.9, weight_decay=5e-4)\n",
    "    elif settings[\"optimizer\"]==\"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=settings[\"lr\"])        \n",
    "    else: print(\"method not defined!!\")\n",
    "\n",
    "    if settings[\"scheduler\"]:\n",
    "        #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100], gamma=0.1)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, int(settings[\"epochs\"]))\n",
    "        \n",
    "    sigma_curr = settings[\"sigma\"]\n",
    "\n",
    "    ########### Setup Writer Variables ###########  \n",
    "    results = {\"rec_steps\":[], \"train_loss\":[], \"test_loss\":[],\"hess\":[], \"test_acc\":[], \"train_acc\":[], \"hess_trace\":[], \"l1_norm\":[], \"l2_norm\":[], \"grad_norm\":[]}    \n",
    "\n",
    "    ########### Getting number of layers ###########      \n",
    "    n_groups = 0\n",
    "    dim_model = 0\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():   \n",
    "            n_groups = n_groups + 1\n",
    "            dim_model = dim_model + torch.numel(param)\n",
    "    print('Model dimension: ' + str(dim_model))\n",
    "    print('Number of groups: ' + str(n_groups))\n",
    "\n",
    "    ##### iteration counter\n",
    "    iter = 0\n",
    "    \n",
    "\t########### Training ###########     \n",
    "    for epoch in range(settings[\"epochs\"]):\n",
    "        start_time = time.time()\n",
    "        print('epoch #'+str(epoch)+', lr = '+str(get_lr(optimizer)))\n",
    "\n",
    "        ########### Computing Hessian if last iteration #####\n",
    "        hess_train = 0\n",
    "        if 0:\n",
    "            if epoch==(settings[\"epochs\"]-1):\n",
    "                model.eval()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                for d in hess_loader:\n",
    "                    data, target = d[0].to(device, non_blocking=True),d[1].to(device, non_blocking=True)\n",
    "                    hess_train = hess_train+np.sum(hessian(model, criterion, data=(data, target), cuda=True).trace())/len(hess_loader) \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        ########### Saving stats every few epochs ########### \n",
    "        if (epoch%settings[\"rec_step\"])==0:\n",
    "            results[\"rec_steps\"].append(epoch)\n",
    "            model.eval()\n",
    "            \n",
    "            #computing stats: train loss\n",
    "            train_loss, correct = 0, 0\n",
    "            for d in train_loader:\n",
    "                data, target = d[0].to(device, non_blocking=True),d[1].to(device, non_blocking=True)\n",
    "                output = model(data)\n",
    "                train_loss += criterion(output, target).data.item()/len(train_loader)\n",
    "                pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "                correct += pred.eq(target.data).cpu().sum()\n",
    "            accuracy_train = 100. * correct.to(torch.float32) / len(train_loader.dataset)\n",
    "\n",
    "            #computing stats: test loss\n",
    "            test_loss, correct = 0, 0\n",
    "            for data, target in validation_loader:\n",
    "                data = data.to(device, non_blocking=True)\n",
    "                target = target.to(device, non_blocking=True)\n",
    "                output = model(data)\n",
    "                #d_out = output.size[-1]\n",
    "                #print(d_out)\n",
    "                test_loss += criterion(output, target).data.item()/len(validation_loader)\n",
    "                pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "                correct += pred.eq(target.data).cpu().sum()\n",
    "            accuracy_test = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "\n",
    "\n",
    "            #regularized loss\n",
    "            # J = torch.zeros((d_out, dim_model))\n",
    "            # for data, target in validation_loader:\n",
    "            #     data = data.to(device, non_blocking=True)\n",
    "            #     target = target.to(device, non_blocking=True)\n",
    "            #     output = model(data)\n",
    "            #     J = J + torch.mean(compute_jacobian(data, output),0) / len(train_loader)            \n",
    "\n",
    "            #saving stats\n",
    "            results[\"train_loss\"].append(train_loss)\n",
    "            results[\"train_acc\"].append(accuracy_train)\n",
    "            results[\"test_loss\"].append(test_loss)\n",
    "            results[\"test_acc\"].append(accuracy_test)\n",
    "            results[\"hess\"].append(hess_train)\n",
    "            results[\"l1_norm\"].append(torch.norm(flat_params(model),1).cpu().detach().numpy())\n",
    "            results[\"l2_norm\"].append(torch.norm(flat_params(model),2).cpu().detach().numpy())\n",
    "            #results[\"grad_norm\"].append(grad_norm(model).numpy())\n",
    "\n",
    "            print('Epoch {}: Train L: {:.4f}, TrainAcc: {:.2f}, Test L: {:.4f}, TestAcc: {:.2f} \\n'.format(epoch, train_loss, accuracy_train, test_loss, accuracy_test))\n",
    "\n",
    "        ########### Saving stats every few epochs ########### \n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            ### model perturbation\n",
    "            if epoch<135:\n",
    "                if settings[\"noise\"] != \"no\":\n",
    "                    param_copy = []\n",
    "                    with torch.no_grad():\n",
    "                        i=0\n",
    "                        for param in model.parameters():\n",
    "                            param_copy.append(param.data)\n",
    "                            if settings[\"noise\"] == \"all\":\n",
    "                                param.data = param.data + (sigma_curr/math.sqrt(n_groups))*torch.normal(0, 1, size=param.size(),device=device)\n",
    "                            elif settings[\"noise\"] == \"layer\":\n",
    "                                if i==(iter%n_groups):\n",
    "                                    param.data = param.data + sigma_curr*torch.normal(0, 1, size=param.size(),device=device)\n",
    "                                i = i+1\n",
    "\n",
    "            ### backprop\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            ### model recovery\n",
    "            if settings[\"noise\"] != \"no\":\n",
    "                with torch.no_grad():\n",
    "                    i=0\n",
    "                    for param in model.parameters():\n",
    "                        param.data = param_copy[i]\n",
    "                        i=i+1\n",
    "\n",
    "            optimizer.step()\n",
    "            #print(loss.item())\n",
    "            iter = iter +1\n",
    "\n",
    "        if settings[\"scheduler\"]:\n",
    "            scheduler.step()\n",
    "        print(time.time()-start_time)\n",
    "\n",
    "    return results\n",
    "\n",
    "def settings_to_str(settings):\n",
    "    return datetime.now().strftime(\"%H_%M_%S\")+ '_' + settings[\"dataset\"] + '_subset' + str(settings[\"subset\"])  + \"_\" + settings[\"net\"] + \"_noise_\" + settings[\"noise\"] + '_bs' + str(settings[\"bs\"]) + '_' + settings[\"optimizer\"] + '_lr' + str(settings[\"lr\"]) + '_sigma'+ str(settings[\"sigma\"])+ '_rec'+ str(settings[\"rec_step\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # example comparing different noise injections to vanilla SGD\n",
    "\n",
    "    #GPU = [2,3]\n",
    "    dataset = \"CIFAR\"\n",
    "    subset = 50000\n",
    "    nep = 50\n",
    "    bs = 128\n",
    "    rec_step = 1\n",
    "    net = \"CIFAR10Res18\"\n",
    "\n",
    "    noise = \"layer\" #no,layer,all\n",
    "    sigma = 0.03\n",
    "    lr1 = 0.01\n",
    "    settings = {\"dataset\":dataset, \"subset\": subset, \"net\": net, \"optimizer\":\"SGD\", \"scheduler\":True, \"noise\":noise, \"bs\":bs, \"lr\":lr1, \"sigma\":sigma, \"epochs\":nep, \"rec_step\":rec_step}\n",
    "    results = train_net(settings)\n",
    "    torch.save(results, 'results'+settings_to_str(settings)+'.pt') \n",
    "\n",
    "    #noise = \"layer\" #layer,all\n",
    "    #sigma = 0.03\n",
    "    #lr1 = 0.01\n",
    "    #settings = {\"dataset\":dataset, \"subset\": subset, \"net\": net, \"optimizer\":\"SGD\", \"scheduler\":True, \"noise\":noise, \"bs\":bs, \"lr\":lr1, \"sigma\":sigma, \"epochs\":nep, \"rec_step\":rec_step}\n",
    "    #results = train_net(settings)\n",
    "    #torch.save(results, 'results'+settings_to_str(settings)+'.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae43cfd-d1f7-49f2-9240-41abeee72a63",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.save(results, 'results'+settings_to_str(settings)+'.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59419688-bd0c-4bd4-8853-481644f137b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
